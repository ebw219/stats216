package edu.lehigh.cse216.lyle.backend;

// Import the Spark package, so that we can make use of the "get" function to
// create an HTTP GET route
import spark.Spark;

// Import Google's JSON library
import com.google.gson.*;
import java.util.Map;
/**
 * For now, our app creates an HTTP server that can only get and add data.
 */
public class App {
    public static void main(String[] args) {

	// gson provides us with a way to turn JSON into objects, and objects
	// into JSON.
	//
	// NB: it must be final, so that it can be accessed from our lambdas
	//
	// NB: Gson is thread-safe.  See
	// https://stackoverflow.com/questions/10380835/is-it-ok-to-use-gson-instance-as-a-static-field-in-a-model-bean-reuse
	final Gson gson = new Gson();

	// dataStore holds all of the data that has been provided via HTTP
	// requests
	//
	// NB: every time we shut down the server, we will lose all data, and
	//     every time we start the server, we'll have an empty dataStore,
	//     with IDs starting over from 0.
	Map<String, String> env = System.getenv();
	String ip = env.get("POSTGRES_IP");
	String port = env.get("POSTGRES_PORT");
	String user = env.get("POSTGRES_USER");
	String pass = env.get("POSTGRES_PASS");

	final Database database = Database.getDatabase(ip, port, user, pass); //changed to database instead of datastore
	if (!database.tableDoesExist()) {
		database.createTable();
	}
	// Set up the location for serving static files.  If the STATIC_LOCATION
	// environment variable is set, we will serve from it.  Otherwise, serve
	// from "/web"
	String static_location_override = System.getenv("STATIC_LOCATION");
	if (static_location_override == null) {
	    Spark.staticFileLocation("/web");
	} else {
	    Spark.staticFiles.externalLocation(static_location_override);
	}

    
	// Set up the location for serving static files
	Spark.staticFileLocation("/web");
	
	// Set up a route for serving the main page
	Spark.get("/", (req, res) -> {
		res.redirect("/index.html");
		return "";
	    });

	// GET route that returns all message titles and Ids.  All we do is get
	// the data, embed it in a StructuredResponse, turn it into JSON, and
	// return it.  If there's no data, we return "[]", so there's no need
	// for error handling.
	Spark.get("/messages", (request, response) -> {
		// ensure status 200 OK, with a MIME type of JSON
		response.status(200);
		response.type("application/json");
		return gson.toJson(new StructuredResponse("ok", null, database.selectAll())); // changed 
	    });

	// GET route that returns everything for a single row in the DataStore.
	// The ":id" suffix in the first parameter to get() becomes
	// request.params("id"), so that we can get the requested row ID.  If
	// ":id" isn't a number, Spark will reply with a status 500 Internal
	// Server Error.  Otherwise, we have an integer, and the only possible
	// error is that it doesn't correspond to a row with data.
	Spark.get("/messages/:id", (request, response) -> {
		int idx = Integer.parseInt(request.params("id"));
		// ensure status 200 OK, with a MIME type of JSON
		response.status(200);
		response.type("application/json");
		Database.RowData data = database.selectOne(idx);
		if (data == null) {
		    return gson.toJson(new StructuredResponse("error", idx + " not found", null));
		} else {
		    return gson.toJson(new StructuredResponse("ok", null, data));
		}
	    });

	// POST route for adding a new element to the DataStore.  This will read
	// JSON from the body of the request, turn it into a SimpleRequest
	// object, extract the title and message, insert them, and return the
	// ID of the newly created row.
	Spark.post("/messages", (request, response) -> {
		// NB: if gson.Json fails, Spark will reply with status 500 Internal
		// Server Error
		SimpleRequest req = gson.fromJson(request.body(), SimpleRequest.class);
		// ensure status 200 OK, with a MIME type of JSON
		// NB: even on error, we return 200, but with a JSON object that
		//     describes the error.
		response.status(200);
		response.type("application/json");
		// NB: createEntry checks for null title and message
		int newId = database.insertRow(req.mTitle, req.mMessage);
		if (newId == -1) {
		    return gson.toJson(new StructuredResponse("error", "error performing insertion", null));
		} else {
		    return gson.toJson(new StructuredResponse("ok", "" + newId, null));
		}
	    });
 

	// PUT route for updating a row in the DataStore.  This is almost
	// exactly the same as POST
	Spark.put("/messages/:id", (request, response) -> {
		// If we can't get an ID or can't parse the JSON, Spark will send
		// a status 500
		int idx = Integer.parseInt(request.params("id"));
		SimpleRequest req = gson.fromJson(request.body(), SimpleRequest.class);
		// ensure status 200 OK, with a MIME type of JSON
		response.status(200);
		response.type("application/json");
		int result = database.updateOne(idx, req.mTitle, req.mMessage);
		if (result == -1) {
		    return gson.toJson(new StructuredResponse("error", "unable to update row " + idx, null));
		} else {
		    return gson.toJson(new StructuredResponse("ok", null, result));
		}
	    });
	
	// DELETE route for removing a row from the DataStore
	Spark.delete("/messages/:id", (request, response) -> {
		// If we can't get an ID, Spark will send a status 500
		int idx = Integer.parseInt(request.params("id"));
		// ensure status 200 OK, with a MIME type of JSON
		response.status(200);
		response.type("application/json");
		// NB: we won't concern ourselves too much with the quality of the
		//     message sent on a successful delete
		int result = database.deleteRow(idx);
		if (result == -1) {
		    return gson.toJson(new StructuredResponse("error", "unable to delete row " + idx, null));
		} else {
		    return gson.toJson(new StructuredResponse("ok", null, null));
		}
	    });

    }

    

}
